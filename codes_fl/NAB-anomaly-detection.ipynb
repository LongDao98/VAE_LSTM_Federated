{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import plot, ion, show, savefig, cla, figure\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from data_loader import DataGenerator\n",
    "from models import VAEmodel, lstmKerasModel\n",
    "from trainers import vaeTrainer\n",
    "\n",
    "from utils import process_config, create_dirs, get_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load VAE model\n",
    "config = process_config('NAB_config3.json')\n",
    "# create the experiments dirs\n",
    "create_dirs([config['result_dir'], config['checkpoint_dir']])\n",
    "# create tensorflow session\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# create your data generator\n",
    "data = DataGenerator(config)\n",
    "# create a CNN model\n",
    "model_vae = VAEmodel(config, \"Global\")\n",
    "# create a CNN model\n",
    "trainer_vae = vaeTrainer(sess, model_vae, data, config)\n",
    "model_vae.load(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load LSTM model\n",
    "lstm_model = lstmKerasModel(\"Global\",config)\n",
    "lstm_model.produce_embeddings(model_vae, data, sess)\n",
    "lstm_nn_model = lstm_model.create_lstm_model()\n",
    "lstm_nn_model.summary()   # Display the model's architecture\n",
    "\n",
    "# checkpoint path\n",
    "checkpoint_path = config['checkpoint_dir_lstm'] + \"cp_Global.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "# load weights if possible\n",
    "lstm_model.load_model(lstm_nn_model, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load normalised time series\n",
    "save_dir = '../datasets/NAB-known-anomaly/'\n",
    "dataset = config['dataset']\n",
    "filename = '{}.npz'.format(dataset)\n",
    "result = dict(np.load(save_dir+filename, allow_pickle=True))\n",
    "if dataset == 'machine_temp':\n",
    "    result['test'] = result['test'][0]\n",
    "    result['idx_anomaly_test'] = result['idx_anomaly_test'][0]\n",
    "    result['t_test'] = result['t_test'][0]\n",
    "\n",
    "# slice into rolling windows and rolling sequences\n",
    "def slice_rolling_windows_and_sequences(config, time_seq):\n",
    "    n_sample = len(time_seq)\n",
    "    time_seq = np.reshape(time_seq,(-1,config['n_channel']))\n",
    "    print(\"The given sequence has {} samples\".format(n_sample))\n",
    "    n_vae_win = n_sample - config['l_win'] + 1\n",
    "    rolling_windows = np.zeros((n_vae_win, config['l_win'], config['n_channel']))\n",
    "    for i in range(n_vae_win):\n",
    "        rolling_windows[i] = time_seq[i:i + config['l_win']]\n",
    "        sample_m = np.mean(rolling_windows, axis=1)\n",
    "        sample_std = np.std(rolling_windows, axis=1)\n",
    "\n",
    "        n_lstm_seq = n_sample - config['l_seq']*config['l_win']+1\n",
    "        lstm_seq = np.zeros((n_lstm_seq, config['l_seq'], config['l_win'], config['n_channel']))\n",
    "    for i in range(n_lstm_seq):\n",
    "        cur_seq = time_seq[i:i+config['l_seq']*config['l_win']]\n",
    "        for j in range(config['l_seq']):\n",
    "            lstm_seq[i,j] = cur_seq[config['l_win']*j:config['l_win']*(j+1)]\n",
    "    \n",
    "    return rolling_windows, lstm_seq, sample_m, sample_std\n",
    "\n",
    "test_windows, test_seq, test_sample_m, test_sample_std = slice_rolling_windows_and_sequences(config, result['test'])\n",
    "test_windows = np.reshape(test_windows, (-1,config['l_win'],config['n_channel']))\n",
    "test_seq = np.reshape(test_seq, (-1,config['l_seq'],config['l_win'],config['n_channel']))\n",
    "print(test_windows.shape)\n",
    "print(test_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.val_set_lstm['data'][23].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ELBO and LSTM prediction error on the validation set\n",
    "# evaluate some anomaly detection metrics\n",
    "def evaluate_vae_anomaly_metrics_for_a_window(test_win):\n",
    "    feed_dict = {model_vae.original_signal: np.expand_dims(test_win, 0),\n",
    "                 model_vae.is_code_input: False,\n",
    "                 model_vae.code_input: np.zeros((1, config['code_size']))}\n",
    "\n",
    "    # VAE reconstruction error\n",
    "    recons_win_vae = np.squeeze(sess.run(model_vae.decoded, feed_dict=feed_dict))\n",
    "    test_vae_recons_error = np.sum(np.square(recons_win_vae - test_win))\n",
    "\n",
    "    # VAE latent embedding likelihood\n",
    "    vae_code_mean, vae_code_std = sess.run([model_vae.code_mean, model_vae.code_std_dev], feed_dict=feed_dict)\n",
    "    test_vae_kl = 0.5 * (np.sum(np.square(vae_code_mean)) + \\\n",
    "                            np.sum(np.square(vae_code_std)) - \\\n",
    "                            np.sum(np.log(np.square(vae_code_std))) - config['code_size'])\n",
    "\n",
    "    # VAE ELBO loss\n",
    "    sigma2 = 0.0005\n",
    "    input_dims = model_vae.input_dims\n",
    "    sigma_regularisor = input_dims/2. * np.log(sigma2) + input_dims * np.pi\n",
    "    test_vae_elbo = test_vae_recons_error/sigma2 + test_vae_kl + sigma_regularisor\n",
    "    return test_vae_recons_error, test_vae_kl, test_vae_elbo\n",
    "\n",
    "def evaluate_lstm_anomaly_metric_for_a_seq(test_seq):\n",
    "    feed_dict = {model_vae.original_signal: test_seq,\n",
    "                 model_vae.is_code_input: False,\n",
    "                 model_vae.code_input: np.zeros((1, config['code_size']))}\n",
    "    vae_embedding = np.squeeze(sess.run(model_vae.code_mean, feed_dict=feed_dict))\n",
    "    #print(vae_embedding.shape)\n",
    "    lstm_embedding = np.squeeze(lstm_nn_model.predict(np.expand_dims(vae_embedding[:config['l_seq']-1], 0), batch_size=1))\n",
    "    lstm_embedding_error = np.sum(np.square(vae_embedding[1:] - lstm_embedding))\n",
    "    error_original = vae_embedding[1:] - lstm_embedding #them dong nay de tinh OCSVM\n",
    "    \n",
    "    # LSTM prediction error\n",
    "    feed_dict_lstm = {model_vae.original_signal: np.zeros((config['l_seq'] - 1, config['l_win'], config['n_channel'])), #them n_channel o day\n",
    "                      model_vae.is_code_input: True,\n",
    "                      model_vae.code_input: lstm_embedding}\n",
    "    recons_win_lstm = np.squeeze(sess.run(model_vae.decoded, feed_dict=feed_dict_lstm))\n",
    "    lstm_recons_error = np.sum(np.square(recons_win_lstm - np.squeeze(test_seq[1:])))\n",
    "    return lstm_recons_error, lstm_embedding_error, error_original\n",
    "\n",
    "n_val_vae = data.val_set_vae['data'].shape[0]\n",
    "n_val_lstm = data.val_set_lstm['data'].shape[0]\n",
    "\n",
    "val_vae_recons_error = np.zeros(n_val_vae)\n",
    "val_vae_kl_error = np.zeros(n_val_vae)\n",
    "val_vae_elbo_loss = np.zeros(n_val_vae)\n",
    "for i in range(n_val_vae):\n",
    "    val_vae_recons_error[i], val_vae_kl_error[i], val_vae_elbo_loss[i] = evaluate_vae_anomaly_metrics_for_a_window(data.val_set_vae['data'][i])\n",
    "\n",
    "val_lstm_recons_error, val_lstm_embedding_error = np.zeros(n_val_lstm), np.zeros(n_val_lstm)\n",
    "val_lstm_error_original = np.zeros((n_val_lstm,config['l_seq']-1,config['code_size'])) #them de tinh OCSVM\n",
    "for i in range(n_val_lstm):\n",
    "    val_lstm_recons_error[i], val_lstm_embedding_error[i], val_lstm_error_original[i] = evaluate_lstm_anomaly_metric_for_a_seq(data.val_set_lstm['data'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def plot_histogram(test_anomaly_metric, n_bins, title, mean=None, std=None, xlim=None, saveplot=False):\n",
    "    test_anomaly_list = np.squeeze(np.ndarray.flatten(test_anomaly_metric))\n",
    "    his = plt.hist(test_anomaly_list, bins=n_bins, density=True)\n",
    "    if mean is None and std is None:\n",
    "        mean = np.mean(test_anomaly_list)\n",
    "        std = np.std(test_anomaly_list)\n",
    "        legend_label = None\n",
    "    else:\n",
    "        legend_label = 1\n",
    "    x_axis = np.arange(mean-5*std, mean+5*std, std/100)\n",
    "    plt.plot(x_axis, norm.pdf(x_axis,mean,std))\n",
    "    plt.title(title)\n",
    "    plt.xlabel('anomaly score value')\n",
    "    plt.ylabel('probability density')\n",
    "    if xlim is not None:\n",
    "        plt.xlim(0, xlim)\n",
    "    else:\n",
    "        plt.xlim(0, np.amax(test_anomaly_list))\n",
    "    if legend_label is None:\n",
    "        plt.legend(('Fitted Gaussian', 'histogram'))\n",
    "    else:\n",
    "        plt.legend(('normal data distribution','test data distribution (contain anomalies)'))\n",
    "    if saveplot:\n",
    "        savefig(config['result_dir']+'reconstruction_error_histogram.pdf')\n",
    "    else:\n",
    "        plt.show()\n",
    "    threshold_25 = np.percentile(test_anomaly_list, 25)\n",
    "    threshold_75 = np.percentile(test_anomaly_list, 75)\n",
    "    threshold_1 = np.percentile(test_anomaly_list, 99)\n",
    "    idx_large_error = np.squeeze(np.argwhere(test_anomaly_metric > threshold_1))\n",
    "#     print(his[0][-20:])\n",
    "#     print(his[1][-20:])\n",
    "    print(\"25% percentile: {}\".format(threshold_25))\n",
    "    print(\"75% percentile: {}\".format(threshold_75))\n",
    "#     print(\"Median: {}\".format(np.median(test_anomaly_list)))\n",
    "#     print(\"Mean: {}\".format(np.mean(test_anomaly_list)))\n",
    "#     print(\"Std dev: {}\".format(np.std(test_anomaly_list)))\n",
    "    print(\"These windows scored the top 1% of anomaly metric ({}): \\n{}\".format(threshold_1, idx_large_error))\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of VAE ELBO loss - validation set\n",
    "vae_elbo_m, vae_elbo_std = plot_histogram(val_vae_elbo_loss, 100, \n",
    "                                          'VAE ELBO error distribution on the val set', \n",
    "                                          mean=None, std=None, xlim=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of LSTM reconstruction error - validation set \n",
    "#  --> to decide the anomaly detection threshold\n",
    "lstm_recons_m, lstm_recons_std = plot_histogram(val_lstm_recons_error, 100,  \n",
    "                                              'LSTM reconstruction error distribution on the val set', \n",
    "                                              mean=None, std=None, xlim=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the anomaly metrics on the test windows and sequences\n",
    "n_test_lstm = test_seq.shape[0]\n",
    "\n",
    "test_lstm_recons_error, test_lstm_embedding_error = np.zeros(n_test_lstm), np.zeros(n_test_lstm)\n",
    "test_lstm_error_original = np.zeros((n_test_lstm,config['l_seq']-1,config['code_size'])) #them de tinh OCSVM\n",
    "for i in range(n_test_lstm):\n",
    "    test_lstm_recons_error[i], test_lstm_embedding_error[i], test_lstm_error_original[i] = evaluate_lstm_anomaly_metric_for_a_seq(test_seq[i])\n",
    "print(\"All windows' reconstruction error is computed.\")\n",
    "print(\"The total number of windows is {}\".format(len(test_lstm_recons_error)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of LSTM reconstruction error - test set \n",
    "#  --> to detect anomaly now\n",
    "_, _ = plot_histogram(test_lstm_recons_error, 100,\n",
    "                      'LSTM reconstruction error distribution on the test set', \n",
    "                      mean=lstm_recons_m, std=lstm_recons_std, xlim=None, saveplot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the ground truth anomaly indices \n",
    "# if result['idx_split'][0] == 0:\n",
    "#     idx_anomaly_test = result['idx_anomaly_test']\n",
    "# else:\n",
    "#     idx_anomaly_test = result['idx_anomaly_test'][0]\n",
    "idx_anomaly_test = result['idx_anomaly_test']    \n",
    "anomaly_index_lstm = []\n",
    "test_labels_lstm = np.zeros(n_test_lstm)\n",
    "for i in range(len(idx_anomaly_test)):\n",
    "    idx_start = idx_anomaly_test[i]-(config['l_win']*config['l_seq']-1)\n",
    "    idx_end = idx_anomaly_test[i]+1\n",
    "    if idx_start < 0:\n",
    "        idx_start = 0\n",
    "    if idx_end > n_test_lstm:\n",
    "        idx_end = n_test_lstm\n",
    "    anomaly_index_lstm.append(np.arange(idx_start,idx_end))\n",
    "    test_labels_lstm[int(idx_start):int(idx_end)] = 1\n",
    "    \n",
    "print(test_labels_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_normal_idx = np.squeeze(np.argwhere(test_labels_lstm==0))\n",
    "test_anomaly_idx = np.squeeze(np.argwhere(test_labels_lstm==1))\n",
    "test_lstm_recons_error_normal = test_lstm_recons_error[test_normal_idx]\n",
    "test_lstm_recons_error_anomaly = test_lstm_recons_error[test_anomaly_idx]\n",
    "lstm_recons_m_test_normal = np.mean(test_lstm_recons_error_normal)\n",
    "lstm_recons_std_test_normal = np.std(test_lstm_recons_error_normal)\n",
    "lstm_recons_m_test_anomaly = np.mean(test_lstm_recons_error_anomaly)\n",
    "lstm_recons_std_test_anomaly = np.std(test_lstm_recons_error_anomaly)\n",
    "lstm_recons_m_test = np.mean(test_lstm_recons_error)\n",
    "lstm_recons_std_test = np.mean(test_lstm_recons_error)\n",
    "\n",
    "means = (lstm_recons_m_test_normal, lstm_recons_m_test_anomaly, lstm_recons_m_test, lstm_recons_m_train, lstm_recons_m_val)\n",
    "stds = (lstm_recons_std_test_normal, lstm_recons_std_test_anomaly, lstm_recons_std_test, lstm_recons_std_train, lstm_recons_std_val)\n",
    "for mean,std in zip(means,stds):\n",
    "    x_axis = np.arange(mean-5*std, mean+5*std, std/100)\n",
    "    #x_axis = np.arange(-10000,10000)\n",
    "    plt.plot(x_axis, norm.pdf(x_axis,mean,std))\n",
    "plt.legend(('test_norm', 'test_anomaly', 'test', 'train', 'val')) \n",
    "#plt.xscale('symlog')\n",
    "#plt.yscale('log')\n",
    "plt.title(\"Distribution of different sets on \" + config['dataset'])\n",
    "#print(lstm_recons_m_test,lstm_recons_std_test)\n",
    "#print(lstm_recons_m_val,lstm_recons_std_val)\n",
    "#print(lstm_recons_m_test_normal,lstm_recons_m_test_normal)\n",
    "print(means)\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_anomaly_idx_by_threshold(test_anomaly_metric, threshold):\n",
    "    test_list = np.squeeze(np.ndarray.flatten(test_anomaly_metric))\n",
    "    idx_error = np.squeeze(np.argwhere(test_anomaly_metric > threshold))\n",
    "    \n",
    "    if len(idx_error.shape) == 0:\n",
    "        idx_error = np.expand_dims(idx_error, 0)\n",
    "    \n",
    "    return list(idx_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_detected_idx(idx_detected_anomaly, anomaly_index):\n",
    "    n_anomaly = len(anomaly_index)\n",
    "    idx_detected_anomaly_extended = list(idx_detected_anomaly)\n",
    "    for i in range(n_anomaly):\n",
    "        #print(idx_detected_anomaly)\n",
    "        for j in idx_detected_anomaly:\n",
    "            if j in anomaly_index[i]:\n",
    "                in_original_detection = set(idx_detected_anomaly_extended)\n",
    "                currect_anomaly_win = set(anomaly_index[i])\n",
    "                idx_detected_anomaly_extended = idx_detected_anomaly_extended + list(currect_anomaly_win - in_original_detection)\n",
    "                #print(j)\n",
    "                break\n",
    "                \n",
    "    return list(np.sort(idx_detected_anomaly_extended))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_TP_FP_FN(idx_detected_anomaly, anomaly_index, test_labels):\n",
    "    n_TP = 0\n",
    "    n_FP = 0\n",
    "    n_detection = len(idx_detected_anomaly)\n",
    "    for i in range(n_detection):\n",
    "        if test_labels[idx_detected_anomaly[i]] == 1:\n",
    "            n_TP = n_TP + 1\n",
    "        else:\n",
    "            n_FP = n_FP + 1 #both branch the same?\n",
    "    \n",
    "    idx_undetected = list(set(np.arange(len(test_labels)))- set(idx_detected_anomaly))\n",
    "    n_FN = 0\n",
    "    for i in idx_undetected:\n",
    "        if test_labels[i] == 1:\n",
    "            n_FN = n_FN + 1\n",
    "    \n",
    "    return n_TP, n_FP, n_FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_and_recall(idx_detected_anomaly, anomaly_index, test_labels):\n",
    "    # compute true positive\n",
    "    n_TP, n_FP, n_FN = count_TP_FP_FN(idx_detected_anomaly, anomaly_index, test_labels)\n",
    "    \n",
    "    if n_TP + n_FP == 0:\n",
    "        precision = 1\n",
    "    else:\n",
    "        precision = n_TP / (n_TP + n_FP)\n",
    "    recall = n_TP / (n_TP + n_FN)\n",
    "    if precision + recall == 0:\n",
    "        F1 = 0\n",
    "    else:\n",
    "        F1 = 2* (precision * recall)/(precision + recall)\n",
    "    \n",
    "    return precision, recall, F1, n_TP, n_FP, n_FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_threshold = 25\n",
    "precision = np.zeros(n_threshold)\n",
    "recall = np.zeros(n_threshold)\n",
    "F1 = np.zeros(n_threshold)\n",
    "precision_aug = np.zeros(n_threshold)\n",
    "recall_aug = np.zeros(n_threshold)\n",
    "F1_aug = np.zeros(n_threshold)\n",
    "i = 0\n",
    "threshold_list = np.linspace(np.amin(test_lstm_recons_error), np.amax(test_lstm_recons_error), n_threshold, endpoint=True)\n",
    "threshold_list = np.flip(threshold_list)\n",
    "for threshold in threshold_list:\n",
    "    #print(threshold_list[i])\n",
    "    idx_detection_lstm = return_anomaly_idx_by_threshold(test_lstm_recons_error, threshold)\n",
    "    precision[i], recall[i], F1[i], _, _, _ = compute_precision_and_recall(idx_detection_lstm, \n",
    "                                                                           anomaly_index_lstm, \n",
    "                                                                           test_labels_lstm)\n",
    "    # augment the detection using the ground truth labels\n",
    "    # a method to discount the factor one anomaly appears in multiple consecutive windows\n",
    "    # introduced in \"Unsupervised anomaly detection via variational auto-encoder for seasonal kpis in web applications\"\n",
    "    idx_detection_lstm_augmented = augment_detected_idx(idx_detection_lstm, anomaly_index_lstm)\n",
    "    precision_aug[i], recall_aug[i], F1_aug[i], _, _, _ = compute_precision_and_recall(idx_detection_lstm_augmented, \n",
    "                                                                                       anomaly_index_lstm, \n",
    "                                                                                       test_labels_lstm)\n",
    "    i = i + 1\n",
    "    #print(precision, recall, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best F1 score is {}\".format(np.amax(F1)))\n",
    "idx_best_threshold = np.squeeze(np.argwhere(F1 == np.amax(F1)))\n",
    "print(\"Best threshold is {}\".format(threshold_list[idx_best_threshold]))\n",
    "print(\"At this threshold, precision is {}, recall is {}\".format(precision[idx_best_threshold], recall[idx_best_threshold]))\n",
    "average_precision = np.sum(precision[1:] * (recall[1:] - recall[:-1]))\n",
    "print(\"Average precision is {}\".format(average_precision))\n",
    "\n",
    "print(\"\\nAugmented detection:\")\n",
    "print(\"Best F1 score is {}\".format(np.amax(F1_aug)))\n",
    "idx_best_threshold = np.squeeze(np.argwhere(F1_aug == np.amax(F1_aug)))\n",
    "print(\"Best threshold is {}\".format(threshold_list[idx_best_threshold]))\n",
    "print(\"At this threshold, precision is {}, recall is {}\".format(precision_aug[idx_best_threshold], \n",
    "                                                                recall_aug[idx_best_threshold]))\n",
    "\n",
    "average_precision_aug = np.sum(precision_aug[1:] * (recall_aug[1:] - recall_aug[:-1]))\n",
    "print(\"Average precision is {}\".format(average_precision_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now select a threshold\n",
    "threshold = 280.32168660405443\n",
    "\n",
    "print(\"Threshold is {}\".format(threshold))\n",
    "idx_detection = return_anomaly_idx_by_threshold(test_lstm_recons_error, threshold)\n",
    "idx_detection_augmented = augment_detected_idx(idx_detection, anomaly_index_lstm)\n",
    "precision, recall, F1, n_TP, n_FP, n_FN = compute_precision_and_recall(idx_detection_augmented, \n",
    "                                                                       anomaly_index_lstm, \n",
    "                                                                       test_labels_lstm)\n",
    "print(\"\\nPR evaluation using augmented detection:\")\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "print(\"F1: {}\".format(F1))\n",
    "print(\"TP: {}\".format(n_TP))\n",
    "print(\"FP: {}\".format(n_FP))\n",
    "print(\"FN: {}\".format(n_FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_detected_indices_into_seq(idx_detection, interval):\n",
    "    detected_seq = []\n",
    "    i = 0\n",
    "    while i < len(idx_detection):\n",
    "        if i == 0:\n",
    "            cur_seq = [idx_detection[i]]\n",
    "            i = i + 1\n",
    "        else:\n",
    "            if idx_detection[i] - idx_detection[i-1] > interval:\n",
    "                detected_seq.append(cur_seq)\n",
    "                cur_seq = [idx_detection[i]]\n",
    "            else:\n",
    "                cur_seq.append(idx_detection[i])\n",
    "                if i == len(idx_detection) - 1:\n",
    "                    detected_seq.append(cur_seq)\n",
    "            i = i + 1\n",
    "    \n",
    "    print(\"Detected {} sequences\".format(len(detected_seq)))\n",
    "    return detected_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detected_anomalies(idx_detection, interval, dataset, result, detection_method, augmented_flag=1, y_scale=5, y_lim=None):\n",
    "    detected_seq = slice_detected_indices_into_seq(idx_detection, interval=interval)\n",
    "    #t_test = result['t_test']\n",
    "    test = result['test']\n",
    "    t_test = np.array(range(np.shape(test)[0]))\n",
    "    idx_anomaly_test = result['idx_anomaly_test']\n",
    "        \n",
    "    # plot detected sequences\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(18, 5), edgecolor='k')\n",
    "    fig.subplots_adjust(hspace=.4, wspace=.4)\n",
    "    axs.plot(t_test, test)\n",
    "    for j in range(len(idx_anomaly_test)):\n",
    "        if j == 0:\n",
    "            axs.plot(idx_anomaly_test[j] * np.ones(20), np.linspace(-y_scale, y_scale, 20), 'r--', label='true anomalies')\n",
    "        else:\n",
    "            axs.plot(idx_anomaly_test[j] * np.ones(20), np.linspace(-y_scale, y_scale, 20), 'r--')\n",
    "        \n",
    "    for i in range(len(detected_seq)):\n",
    "        for j in detected_seq[i]:\n",
    "            if j == detected_seq[0][0]:\n",
    "                axs.plot((j+interval*2) * np.ones(20), np.linspace(-y_scale, -0.8*y_scale, 20), 'g-', label='detected anomalies')\n",
    "            else:\n",
    "                axs.plot((j+interval*2) * np.ones(20), np.linspace(-y_scale, -0.8*y_scale, 20), 'g-')\n",
    "    \n",
    "    for j in range(len(idx_anomaly_test)):\n",
    "        axs.plot(idx_anomaly_test[j] * np.ones(20), np.linspace(-y_scale, y_scale, 20), 'r--')\n",
    "\n",
    "    for i in range(len(detected_seq)):\n",
    "        interval_x = np.asarray([detected_seq[i][0], detected_seq[i][-1]+interval*2])\n",
    "        interval_y = np.asarray([y_scale,y_scale])\n",
    "        if i == 0:\n",
    "            axs.fill_between(interval_x, interval_y, alpha=0.2, color='y', label='detected anomaly windows')\n",
    "        else:\n",
    "            axs.fill_between(interval_x, interval_y, alpha=0.2, color='y')\n",
    "        interval_y = np.asarray([-y_scale,-y_scale])\n",
    "        axs.fill_between(interval_x, interval_y, alpha=0.2, color='y')\n",
    "    axs.grid(True)\n",
    "    axs.set_xlim(0, len(t_test))\n",
    "    if y_lim is None:\n",
    "        axs.set_ylim(-y_scale, y_scale)\n",
    "    else:\n",
    "        axs.set_ylim(-y_scale, y_lim)\n",
    "    axs.set_xlabel(\"timestamp (every {})\".format(result['t_unit']))\n",
    "    axs.set_ylabel(\"normalised readings\")\n",
    "    axs.set_title(\"{} dataset test sequence\\n(normalised by train mean {:.4f} and std {:.4f})\\n Detection method: {}\".format(dataset, \n",
    "                                                                                        np.mean(result['train_m']), \n",
    "                                                                                        np.mean(result['train_std']),\n",
    "                                                                                        detection_method))\n",
    "    axs.legend()\n",
    "    savefig(config['result_dir']+'detected_anomalies_{}_aug_{}.pdf'.format(detection_method, augmented_flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_detected_anomalies(idx_detection_augmented, \n",
    "                        interval=config['l_win']*config['l_seq']/2, \n",
    "                        dataset=dataset, \n",
    "                        result=result, \n",
    "                        detection_method='lstm reconstruction error',\n",
    "                        augmented_flag=1,\n",
    "                        y_scale=5,\n",
    "                        y_lim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OCSVM\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#X_train =\n",
    "#predicted_train =\n",
    "#X_test = \n",
    "#predicted = \n",
    "y_test1 = np.where(test_labels_lstm==1,-1,1) #doi lai label\n",
    "\n",
    "#OCSVM\n",
    "#e=X_train - predicted_train\n",
    "e = val_lstm_error_original\n",
    "nsamples, nx, ny = e.shape\n",
    "d2_e = e.reshape((nsamples,nx*ny))\n",
    "from sklearn import svm\n",
    "clf = svm.OneClassSVM(nu=0.0055, kernel=\"rbf\", gamma=1.5)\n",
    "clf.fit(d2_e)\n",
    "\n",
    "#e_t=X_test - predicted\n",
    "e_t = test_lstm_error_original\n",
    "nsamples, nx, ny = e_t.shape\n",
    "\n",
    "d2_e_t = e_t.reshape((nsamples,nx*ny))\n",
    "y_scores = clf.predict(d2_e_t)\n",
    "\n",
    "precision = precision_score(y_test1, y_scores, pos_label=-1)\n",
    "recall    = recall_score(y_test1, y_scores,pos_label=-1)\n",
    "accuracy = accuracy_score(y_test1, y_scores)\n",
    "f1 = f1_score(y_test1, y_scores, pos_label=-1)\n",
    "print ('OCSVM:')\n",
    "print ('Precision : ', precision)\n",
    "print ('Recall: ', recall)\n",
    "print ('Accuracy : ', accuracy)\n",
    "print ('F1_score: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
